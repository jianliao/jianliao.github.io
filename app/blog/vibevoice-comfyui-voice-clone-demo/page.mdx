import BlogHeader from '../BlogTitle';
import FeedbackFooter from '../FeedbackFooter';
import Image from 'next/image';
import minimalComfyuiWorkflow from './voice-clone-workflow.png';
export const metadata = {
  title: "Cloning My Own Voice with VibeVoice and ComfyUI",
  description:
    "How I used the 9B-parameter VibeVoice-Large model in ComfyUI to clone my own voice and generate realistic presentation narration directly from text.",
  openGraph: {
    title: "Cloning My Own Voice with VibeVoice and ComfyUI",
    description:
      "A hands-on walkthrough of using the VibeVoice-Large model inside ComfyUI to create natural English narration for presentations‚Äîcomplete with parameter tips, workflow setup, and audio samples.",
    url: "/blog/vibevoice-comfyui-voice-clone-demo",
    images: [
      {
        url: "/images/vibevoice-comfyui-voice-clone-demo/voice-clone-workflow.png",
        alt: "ComfyUI workflow showing VibeVoice Single Speaker node chain",
      },
    ],
  },
  twitter: {
    title: "Cloning My Own Voice with VibeVoice and ComfyUI",
    images: ["/images/vibevoice-comfyui-voice-clone-demo/voice-clone-workflow.png"],
  },
};

<BlogHeader title="Cloning My Own Voice with VibeVoice and ComfyUI" postDate="2025-10-18" />

## Introduction

Recently, I experimented with using **VibeVoice-Large (9B params)** to generate a realistic English narration for my demo video ‚Äî using **my own voice** as the reference.  

The workflow ran entirely in **ComfyUI**, and I was surprised by how natural and accurate the cloned voice sounded ‚Äî even reproducing subtle traits like my *slight lisp* :D.

---

## My setup

**Environment**

- **ComfyUI** ([latest build](https://github.com/comfyanonymous/ComfyUI/releases))  
- **ComfyUI Manager([3.35](https://github.com/Comfy-Org/ComfyUI-Manager/releases/tag/3.35))**  
- **VibeVoice Custom Node:** [Enemyx-net/VibeVoice-ComfyUI](https://github.com/Enemyx-net/VibeVoice-ComfyUI)

**Model**

- [VibeVoice-Large (Hugging Face)](https://huggingface.co/aoi-ot/VibeVoice-Large)  


**Tools**

- üéß Recording: **OBS**  
- üé¨ Editing: **Adobe Premiere Pro**

### Training audio sample

Here‚Äôs the short voice clip I used as my reference audio for cloning:

<audio controls preload="none">
  <source src="/aduios/Jian_Voice_2.m4a" type="audio/mp4" />
  Your browser does not support the audio element.
</audio>

*(Duration: 1 min)*

## What I learned

Here are a few things I discovered while fine-tuning the workflow and experimenting with prompts and parameters.

### 1. Use lowercase for acronyms
The LLM doesn‚Äôt always pronounce uppercase abbreviations correctly.  
For example, ‚ÄúALT‚Äù might be read as ‚ÄúA ‚Äî L ‚Äî T‚Äù.  
To fix this, write them in lowercase or phonetically (‚Äúalt‚Äù) if you expect a natural pronunciation.

### 2. Match your sample voice to the output context
If your target is a **presentation**, record yourself reading a **presentation-style** passage.  
This helps the model pick up your pacing, tone, and emotional cadence, which leads to better results than using a mismatched sample (like a casual conversation).

### 3. Keep `voice_speed_factor` at 1.0
It‚Äôs tempting to adjust this parameter, but even small changes can hurt output quality.  
If you need faster or slower narration, adjust the playback speed later in **Premiere Pro**.  
Let the model focus on **clarity** and **natural rhythm**.

### 4. Avoid fixed seeds
Surprisingly, using a fixed seed often introduced **unwanted background noise** at the start of the audio.  
Switching to `randomize` produced cleaner, consistent results when using a real-voice reference clip.

### 5. Reference audio makes all the difference
Once I used my own 1-minute voice recording as a guide, the generated voice became almost indistinguishable ‚Äî  
not only capturing tone and timbre, but even small quirks like breath timing and tongue sounds.  
It felt a bit uncanny, but in a good way.

## My minimal ComfyUI workflow

You can reproduce the setup with a simple node chain:

<Image src={minimalComfyuiWorkflow} alt="Minimal ComfyUI workflow" />

## Demo Output

Here‚Äôs the final audio extracted from my **3-minute presentation demo video** generated by VibeVoice:

<audio controls preload="none">
  <source src="/videos/ai-alt-text-generator/spectrum-ai-alt-text-demo.mp4" type="audio/mp4" />
  Your browser does not support the audio element.
</audio>

*(Duration: ~3 min)*

## Final thoughts
Cloning your own voice for AI narration feels like magic ‚Äî
especially when the output sounds authentic enough for a live presentation.
With ComfyUI and VibeVoice, the entire process was visual, controllable, and surprisingly intuitive.

That said, for VibeVoice to become truly production-ready, it still needs a variety of LoRA fine-tunes.
Without them, the model struggles in more complex scenarios ‚Äî for example, reading mathematical or physics formulas smoothly and accurately.
Once domain-specific LoRAs are introduced, VibeVoice could evolve from a demo tool into a highly capable voice synthesis system for creative and technical presentations alike.

<FeedbackFooter />
